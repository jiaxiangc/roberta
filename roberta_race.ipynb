{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RoBERTa trained on RACE dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data4/cjx/miniconda3/envs/race/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 导入环境\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaTokenizer, RobertaForMultipleChoice\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:10<00:00,  1.00s/it]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "for i in tqdm(range(10)):\n",
    "    time.sleep(1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RACE Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RACEDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        # 处理数据集，主要包含RoBERTa的分词器\n",
    "        self.data = []\n",
    "        # print('Load RoBERTa tokenizer.')\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "        # _tqdm_idx = -1\n",
    "        # print('Extract data from file of .txt.')\n",
    "        for root, dirs, files in os.walk(data_dir):\n",
    "            # _tqdm_idx += 1\n",
    "            for file in tqdm(files, desc=f\"Process dataset\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                    data = json.loads(content)\n",
    "\n",
    "                    article = data['article']\n",
    "                    questions = data['questions']\n",
    "                    options = data['options']\n",
    "                    answers = data['answers']\n",
    "\n",
    "                    \n",
    "                    for i in range(len(questions)):\n",
    "                        encoded_inputs = self.tokenizer(\n",
    "                            [article + ' ' + questions[i] for _ in range(len(options[i]))],\n",
    "                            options[i],\n",
    "                            # add_special_tokens=True,\n",
    "                            max_length=512,\n",
    "                            padding='max_length',\n",
    "                            truncation=True,\n",
    "                            # TODO: what\n",
    "                            return_tensors='pt'\n",
    "                        )\n",
    "                        input_ids = encoded_inputs['input_ids']\n",
    "                        attention_mask = encoded_inputs['attention_mask']\n",
    "                        answer = torch.tensor(ord(answers[i]) - ord('A'))\n",
    "\n",
    "                        self.data.append((input_ids, attention_mask, answer))\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 一个Item代表一个文章、一个问题、一个答案\n",
    "        # 说明文章的每个问题，拆分成了一个对应的Item\n",
    "        input_ids, attention_mask, answer = self.data[index]\n",
    "        return input_ids, attention_mask, answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ord('B') - ord('A'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process dataset: 0it [00:00, ?it/s]\n",
      "Process dataset: 100%|██████████| 368/368 [00:06<00:00, 56.20it/s]\n",
      "Process dataset: 100%|██████████| 1021/1021 [00:23<00:00, 43.66it/s]\n"
     ]
    }
   ],
   "source": [
    "data_dir = './RACE/'\n",
    "dev_dataset = RACEDataset(os.path.join(data_dir, 'dev'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 512])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_dataset[0][0].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoBERTa model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoBERTaClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RoBERTaClassifier, self).__init__()\n",
    "        self.roberta = RobertaForMultipleChoice.from_pretrained('roberta-base')\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # TODO: what symbol\n",
    "        logits = outputs.logits\n",
    "        return logits\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train process function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    # model.to(device)\n",
    "    \n",
    "    # TODO: where is epochs\n",
    "    for input_ids, attention_mask, answer in tqdm(train_loader, desc='Train RoBERTa'):\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        answer = answer.to(device)\n",
    "\n",
    "        # 标准化流程\n",
    "        optimizer.zero_grad()\n",
    "        # print(f'input_ids {input_ids.shape}')\n",
    "        # print(f'attention_mask {attention_mask.shape}')\n",
    "\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        print(f'Logits shape: {logits.shape}')\n",
    "        loss = criterion(logits, answer)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f'Batch size loss: {loss.item()}')\n",
    "\n",
    "        \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation process function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, eval_loader, device):\n",
    "    model.eval()\n",
    "    # model.to(device)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, answer in tqdm(eval_loader,desc='Eval RoBERTa'):\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            answer = answer.to(device)\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            _, predicted = torch.max(logits, dim=1)\n",
    "            total += answer.size(0)\n",
    "            correct += (predicted == answer).sum().item()\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process dataset: 0it [00:00, ?it/s]\n",
      "Process dataset: 100%|██████████| 368/368 [00:06<00:00, 55.41it/s]\n",
      "Process dataset: 100%|██████████| 1021/1021 [00:23<00:00, 43.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev dataset done\n",
      "Dev dataloader done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 定义超参数\n",
    "device = torch.device('cuda')\n",
    "data_dir = './RACE/'\n",
    "batch_size = 4\n",
    "num_epochs = 5\n",
    "learning_rate = 2e-5\n",
    "\n",
    "# 加载RACE数据集\n",
    "# train_dataset = RACEDataset(os.path.join(data_dir, 'train'))\n",
    "# print('Train dataset done')\n",
    "dev_dataset = RACEDataset(os.path.join(data_dir, 'dev'))\n",
    "print('Dev dataset done')\n",
    "\n",
    "# 创建DataLoader\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# print('Train dataloader done')\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=batch_size)\n",
    "print('Dev dataloader done')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 创建RoBERTa模型\n",
    "model = RoBERTaClassifier().to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train RoBERTa:   0%|          | 0/1222 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([4, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train RoBERTa:   0%|          | 1/1222 [00:01<29:12,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size loss: 1.4180974960327148\n",
      "Logits shape: torch.Size([4, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train RoBERTa:   0%|          | 2/1222 [00:01<16:13,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size loss: 1.444307804107666\n",
      "Logits shape: torch.Size([4, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train RoBERTa:   0%|          | 3/1222 [00:02<12:00,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size loss: 1.3969972133636475\n",
      "Logits shape: torch.Size([4, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train RoBERTa:   0%|          | 4/1222 [00:02<09:59,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size loss: 1.3857412338256836\n",
      "Logits shape: torch.Size([4, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train RoBERTa:   0%|          | 5/1222 [00:02<08:53,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size loss: 1.3712937831878662\n",
      "Logits shape: torch.Size([4, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train RoBERTa:   0%|          | 6/1222 [00:03<08:14,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size loss: 1.3881642818450928\n",
      "Logits shape: torch.Size([4, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train RoBERTa:   1%|          | 7/1222 [00:03<07:48,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size loss: 1.4206831455230713\n",
      "Logits shape: torch.Size([4, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train RoBERTa:   1%|          | 8/1222 [00:03<07:31,  2.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size loss: 1.3801727294921875\n",
      "Logits shape: torch.Size([4, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train RoBERTa:   1%|          | 9/1222 [00:04<07:20,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size loss: 1.377102017402649\n",
      "Logits shape: torch.Size([4, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train RoBERTa:   1%|          | 10/1222 [00:04<07:12,  2.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size loss: 1.3573033809661865\n",
      "Logits shape: torch.Size([4, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train RoBERTa:   1%|          | 11/1222 [00:04<07:07,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size loss: 1.3728522062301636\n",
      "Logits shape: torch.Size([4, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train RoBERTa:   1%|          | 12/1222 [00:05<07:04,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size loss: 1.321674108505249\n",
      "Logits shape: torch.Size([4, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train RoBERTa:   1%|          | 13/1222 [00:05<07:00,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size loss: 1.4113768339157104\n",
      "Logits shape: torch.Size([4, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train RoBERTa:   1%|          | 14/1222 [00:05<06:59,  2.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size loss: 1.3483734130859375\n",
      "Logits shape: torch.Size([4, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train RoBERTa:   1%|          | 15/1222 [00:06<06:59,  2.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size loss: 1.3091610670089722\n",
      "Logits shape: torch.Size([4, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train RoBERTa:   1%|▏         | 16/1222 [00:06<06:56,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size loss: 1.372544288635254\n",
      "Logits shape: torch.Size([4, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train RoBERTa:   1%|▏         | 17/1222 [00:06<06:55,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size loss: 1.3784289360046387\n",
      "Logits shape: torch.Size([4, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train RoBERTa:   1%|▏         | 18/1222 [00:07<06:56,  2.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size loss: 1.3782680034637451\n",
      "Logits shape: torch.Size([4, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train RoBERTa:   2%|▏         | 19/1222 [00:07<06:55,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size loss: 1.3525745868682861\n",
      "Logits shape: torch.Size([4, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train RoBERTa:   2%|▏         | 20/1222 [00:07<06:52,  2.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size loss: 1.53678560256958\n",
      "Logits shape: torch.Size([4, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train RoBERTa:   2%|▏         | 21/1222 [00:08<06:54,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size loss: 1.3562309741973877\n",
      "Logits shape: torch.Size([4, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train RoBERTa:   2%|▏         | 22/1222 [00:08<06:53,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size loss: 1.178386926651001\n",
      "Logits shape: torch.Size([4, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train RoBERTa:   2%|▏         | 23/1222 [00:09<06:53,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size loss: 1.221478819847107\n",
      "Logits shape: torch.Size([4, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train RoBERTa:   2%|▏         | 24/1222 [00:09<06:52,  2.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size loss: 1.3405234813690186\n",
      "Logits shape: torch.Size([4, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train RoBERTa:   2%|▏         | 25/1222 [00:09<06:51,  2.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size loss: 1.6706300973892212\n",
      "Logits shape: torch.Size([4, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train RoBERTa:   2%|▏         | 26/1222 [00:10<06:51,  2.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size loss: 1.564473271369934\n",
      "Logits shape: torch.Size([4, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train RoBERTa:   2%|▏         | 27/1222 [00:10<06:52,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size loss: 1.616933822631836\n",
      "Logits shape: torch.Size([4, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train RoBERTa:   2%|▏         | 28/1222 [00:10<06:49,  2.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size loss: 1.5064396858215332\n",
      "Logits shape: torch.Size([4, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train RoBERTa:   2%|▏         | 28/1222 [00:11<07:51,  2.53it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m      3\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mnum_epochs\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m     train_model(model, dev_loader, criterion, optimizer, device)\n\u001b[1;32m      5\u001b[0m     accuracy \u001b[39m=\u001b[39m evaluate_model(model, dev_loader, device)\n\u001b[1;32m      6\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mnum_epochs\u001b[39m}\u001b[39;00m\u001b[39m: Accuracy: \u001b[39m\u001b[39m{\u001b[39;00maccuracy\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[18], line 19\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mLogits shape: \u001b[39m\u001b[39m{\u001b[39;00mlogits\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     18\u001b[0m loss \u001b[39m=\u001b[39m criterion(logits, answer)\n\u001b[0;32m---> 19\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     20\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     22\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mBatch size loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m.\u001b[39mitem()\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/data4/cjx/miniconda3/envs/race/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m/data4/cjx/miniconda3/envs/race/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 训练和评估模型\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    train_model(model, dev_loader, criterion, optimizer, device)\n",
    "    accuracy = evaluate_model(model, dev_loader, device)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}: Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 512]' is invalid for input of size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# input_ids = train_dataset[100][0].unsqueeze(0).to(device)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# attention_masks = train_dataset[100][1].unsqueeze(0).to(device)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m logits \u001b[39m=\u001b[39m model(input_ids, attention_masks)\n",
      "File \u001b[0;32m/data4/cjx/miniconda3/envs/race/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m, in \u001b[0;36mRoBERTaClassifier.forward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input_ids, attention_mask):\n\u001b[0;32m----> 7\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroberta(input_ids\u001b[39m=\u001b[39;49minput_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask)\n\u001b[1;32m      8\u001b[0m     \u001b[39m# TODO: what symbol\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     logits \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlogits\n",
      "File \u001b[0;32m/data4/cjx/miniconda3/envs/race/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data4/cjx/miniconda3/envs/race/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:1340\u001b[0m, in \u001b[0;36mRobertaForMultipleChoice.forward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, labels, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1338\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(pooled_output)\n\u001b[1;32m   1339\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(pooled_output)\n\u001b[0;32m-> 1340\u001b[0m reshaped_logits \u001b[39m=\u001b[39m logits\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, num_choices)\n\u001b[1;32m   1342\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1343\u001b[0m \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     \u001b[39m# move labels to correct device to enable model parallelism\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 512]' is invalid for input of size 1"
     ]
    }
   ],
   "source": [
    "input_ids = train_dataset[100][0].unsqueeze(0).to(device)\n",
    "attention_masks = train_dataset[100][1].unsqueeze(0).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 512]' is invalid for input of size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m logits \u001b[39m=\u001b[39m model(input_ids, attention_masks)\n",
      "File \u001b[0;32m/data4/cjx/miniconda3/envs/race/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m, in \u001b[0;36mRoBERTaClassifier.forward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input_ids, attention_mask):\n\u001b[0;32m----> 7\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroberta(input_ids\u001b[39m=\u001b[39;49minput_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask)\n\u001b[1;32m      8\u001b[0m     \u001b[39m# TODO: what symbol\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     logits \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlogits\n",
      "File \u001b[0;32m/data4/cjx/miniconda3/envs/race/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data4/cjx/miniconda3/envs/race/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:1340\u001b[0m, in \u001b[0;36mRobertaForMultipleChoice.forward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, labels, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1338\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(pooled_output)\n\u001b[1;32m   1339\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(pooled_output)\n\u001b[0;32m-> 1340\u001b[0m reshaped_logits \u001b[39m=\u001b[39m logits\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, num_choices)\n\u001b[1;32m   1342\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1343\u001b[0m \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     \u001b[39m# move labels to correct device to enable model parallelism\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 512]' is invalid for input of size 1"
     ]
    }
   ],
   "source": [
    "logits = model(input_ids, attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0,  3762,   325, 10226,     8,   664,  3362,   198,     5,   232,\n",
       "           32,    11,     5,   652,     9,  2086,    49,  1576,    30,  6288,\n",
       "            7,  7337,   930,     4,   152,    16,   309,     7,     5,   623,\n",
       "         1309,  6481,     4, 50118, 41026,   383,    64,   146,    82,  1372,\n",
       "            8,   455,     9,  1007,   101,   205,   930,     4,  1876,    82,\n",
       "          679, 25731,    16,   357,   114,    47,    32,  6288,     7,  3152,\n",
       "            8,  3825,     4,   125,   114,    47,   269,  4161,     7,     5,\n",
       "          930, 24120,     6,   190,   269,   205,   930,     6,    24,    64,\n",
       "         2581,   110,  1576,  7340,     4, 50118,  1106,    10,   621,  1239,\n",
       "           10, 15604,     7,   213,    31,    65,   317,     7,     5,    97,\n",
       "           13,   457,    41,  1946,    11,     5,   662,     8,    10,   457,\n",
       "           41,  1946,    11,     5,  1559,     6,     8,   358,   183,    34,\n",
       "            7, 18134,    15,    39,  6086,  2187,   142,    89,    16,    98,\n",
       "          203,     9,  6496,     9,     5,  2341,     8,   960,   198,     6,\n",
       "            8,    16,  6288,     7,    13,    65,  1946,   358,   183,     6,\n",
       "           39,  1576,    16,   164,     7,    28,  2581,  3640,    11,    10,\n",
       "          367,   107,     6,    11,    10,   891,     9,   107,    86,     6,\n",
       "           13,   686,     4, 50118,   970,    64,    28,   171,  6134,     9,\n",
       "        16031,  1389,     9,  2369,     4,    85,  7971,    15,   141,  7337,\n",
       "            5,  2369,    16,     8,   141,   251,    47,  4161,     7,    24,\n",
       "            4,  1890, 22725,    64,  1266,  6496,  1389,     9,  5663,  5044,\n",
       "         1452,  2507,    13,   799,   722,    10,   183,    50,   727,  5044,\n",
       "         1452,  2507,    13,    95,   379,   728,     4, 50118,   970,    32,\n",
       "         2007,  1319,     7,  1744,    82,    31, 16031,  2369,  1389,     4,\n",
       "         2880,    82,    54,  3568,  5567,  2911, 16148,   148, 12858,    64,\n",
       "         2254,   930,    23,  1814,  5044,  1452,  2507,    25,   203,    25,\n",
       "           51,    64,    23,  8017,  5044,  1452,  2507,     4,   125,  5567,\n",
       "         2911, 16148,   189,    45,   356,   182,  3035,     4,    20,   754,\n",
       "           14,  5567,  2911, 16148,   189,   356,   542,    12, 24336,   189,\n",
       "           28,  1528,   452,     6,    53,   114,    89,    16,  1085,  1593,\n",
       "           19,   110,  1576,    11,     5,   499,   189,    28,  1528,     8,\n",
       "         2498,  5567,  2911, 16148,   189,   888,    28,  3035,     4, 50118,\n",
       "        21518,  1537, 10791,    16,     7,  1004,   159,     5,  3149,    15,\n",
       "          110,  1081,  6086,  2110,     4,    20,   623,  1309,  6481,    67,\n",
       "        18192,   664,    82,     7,  3000,    49,   304,     9,   215,  2110,\n",
       "            7,   540,    87,    65,  1946,    10,   183,     4, 50118, 41107,\n",
       "            6,     5,   623,  1309,  6481, 14236,    82,     7,   304,   806,\n",
       "            6,   215,    25,  2793,  6086,  2110,   489,  6288,  1389,  1522,\n",
       "            4,     2,     2,   713,  9078, 19197,  1437,  1437, 18134,  1437,\n",
       "         1437,  1319,     7,  1744,    82,    31, 16031,  2369,  1389,     4,\n",
       "            2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[100][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('race')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ff8ee494585a1875a05112a68bddbbfd9aa592c4e1f7ee722c0946aa5b088794"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

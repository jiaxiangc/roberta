{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RoBERTa trained on RACE dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入环境\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaTokenizer, RobertaForMultipleChoice\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:10<00:00,  1.00s/it]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "for i in tqdm(range(10)):\n",
    "    time.sleep(1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RACE Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RACEDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        # 处理数据集，主要包含RoBERTa的分词器\n",
    "        self.data = []\n",
    "        # print('Load RoBERTa tokenizer.')\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "        # print('Extract data from file of .txt.')\n",
    "        for root, dirs, files in os.walk(data_dir):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                    data = json.loads(content)\n",
    "\n",
    "                    article = data['article']\n",
    "                    questions = data['questions']\n",
    "                    options = data['options']\n",
    "                    answers = data['answers']\n",
    "\n",
    "                    for i in range(len(questions)):\n",
    "                        encoded_inputs = self.tokenizer.encode_plus(\n",
    "                            article,\n",
    "                            questions[i],\n",
    "                            options[i],\n",
    "                            # add_special_tokens=True,\n",
    "                            max_length=512,\n",
    "                            padding='max_length',\n",
    "                            truncation=True,\n",
    "                            # TODO: what\n",
    "                            return_tensors='pt'\n",
    "                        )\n",
    "                        input_ids = encoded_inputs['input_ids'].squeeze()\n",
    "                        attention_mask = encoded_inputs['attention_mask'].squeeze()\n",
    "                        answer = ord(answers[i]) - ord('A')\n",
    "\n",
    "                        self.data.append((input_ids, attention_mask, answer))\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 一个Item代表一个文章、一个问题、一个答案\n",
    "        # 说明文章的每个问题，拆分成了一个对应的Item\n",
    "        input_ids, attention_mask, answer = self.data[index]\n",
    "        return input_ids, attention_mask, answer\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoBERTa model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoBERTaClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RoBERTaClassifier, self).__init__()\n",
    "        self.roberta = RobertaForMultipleChoice.from_pretrained('roberta-base')\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # TODO: what symbol\n",
    "        logits = outputs.logits\n",
    "        return logits\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train process function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    # model.to(device)\n",
    "    \n",
    "    # TODO: where is epochs\n",
    "    for input_ids, attention_mask, answer in tqdm(train_loader):\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        answer = answer.to(device)\n",
    "\n",
    "        # 标准化流程\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        print(f'Logits: {logits}')\n",
    "        loss = criterion(logits, answer)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f'Batch size loss: {loss.item()}')\n",
    "\n",
    "        \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation process function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, eval_loader, device):\n",
    "    model.eval()\n",
    "    # model.to(device)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, answer in eval_loader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            answer = answer.to(device)\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            _, predicted = torch.max(logits, dim=1)\n",
    "            total += answer.size(0)\n",
    "            correct += (predicted == answer).sum().item()\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m learning_rate \u001b[39m=\u001b[39m \u001b[39m2e-5\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[39m# 加载RACE数据集\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m train_dataset \u001b[39m=\u001b[39m RACEDataset(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(data_dir, \u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[1;32m     10\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTrain dataset done\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m dev_dataset \u001b[39m=\u001b[39m RACEDataset(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(data_dir, \u001b[39m'\u001b[39m\u001b[39mdev\u001b[39m\u001b[39m'\u001b[39m))\n",
      "Cell \u001b[0;32mIn[18], line 22\u001b[0m, in \u001b[0;36mRACEDataset.__init__\u001b[0;34m(self, data_dir)\u001b[0m\n\u001b[1;32m     19\u001b[0m answers \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39manswers\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     21\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(questions)):\n\u001b[0;32m---> 22\u001b[0m     encoded_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mencode_plus(\n\u001b[1;32m     23\u001b[0m         article,\n\u001b[1;32m     24\u001b[0m         questions[i],\n\u001b[1;32m     25\u001b[0m         options[i],\n\u001b[1;32m     26\u001b[0m         \u001b[39m# add_special_tokens=True,\u001b[39;49;00m\n\u001b[1;32m     27\u001b[0m         max_length\u001b[39m=\u001b[39;49m\u001b[39m512\u001b[39;49m,\n\u001b[1;32m     28\u001b[0m         padding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmax_length\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     29\u001b[0m         truncation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     30\u001b[0m         \u001b[39m# TODO: what\u001b[39;49;00m\n\u001b[1;32m     31\u001b[0m         return_tensors\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m'\u001b[39;49m\n\u001b[1;32m     32\u001b[0m     )\n\u001b[1;32m     33\u001b[0m     input_ids \u001b[39m=\u001b[39m encoded_inputs[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39msqueeze()\n\u001b[1;32m     34\u001b[0m     attention_mask \u001b[39m=\u001b[39m encoded_inputs[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39msqueeze()\n",
      "File \u001b[0;32m/data4/cjx/miniconda3/envs/race/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2727\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2717\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2718\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2719\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[1;32m   2720\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2724\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2725\u001b[0m )\n\u001b[0;32m-> 2727\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_encode_plus(\n\u001b[1;32m   2728\u001b[0m     text\u001b[39m=\u001b[39;49mtext,\n\u001b[1;32m   2729\u001b[0m     text_pair\u001b[39m=\u001b[39;49mtext_pair,\n\u001b[1;32m   2730\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   2731\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m   2732\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m   2733\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   2734\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   2735\u001b[0m     is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   2736\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   2737\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   2738\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   2739\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   2740\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   2741\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   2742\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   2743\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   2744\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   2745\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2746\u001b[0m )\n",
      "File \u001b[0;32m/data4/cjx/miniconda3/envs/race/lib/python3.9/site-packages/transformers/tokenization_utils.py:649\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[39mif\u001b[39;00m return_offsets_mapping:\n\u001b[1;32m    641\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    642\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    643\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 649\u001b[0m first_ids \u001b[39m=\u001b[39m get_input_ids(text)\n\u001b[1;32m    650\u001b[0m second_ids \u001b[39m=\u001b[39m get_input_ids(text_pair) \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    652\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_for_model(\n\u001b[1;32m    653\u001b[0m     first_ids,\n\u001b[1;32m    654\u001b[0m     pair_ids\u001b[39m=\u001b[39msecond_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    668\u001b[0m     verbose\u001b[39m=\u001b[39mverbose,\n\u001b[1;32m    669\u001b[0m )\n",
      "File \u001b[0;32m/data4/cjx/miniconda3/envs/race/lib/python3.9/site-packages/transformers/tokenization_utils.py:617\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus.<locals>.get_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(text, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    616\u001b[0m     tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenize(text, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 617\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_tokens_to_ids(tokens)\n\u001b[1;32m    618\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(text, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(text) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(text[\u001b[39m0\u001b[39m], \u001b[39mstr\u001b[39m):\n\u001b[1;32m    619\u001b[0m     \u001b[39mif\u001b[39;00m is_split_into_words:\n",
      "File \u001b[0;32m/data4/cjx/miniconda3/envs/race/lib/python3.9/site-packages/transformers/tokenization_utils.py:579\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.convert_tokens_to_ids\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    577\u001b[0m ids \u001b[39m=\u001b[39m []\n\u001b[1;32m    578\u001b[0m \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m tokens:\n\u001b[0;32m--> 579\u001b[0m     ids\u001b[39m.\u001b[39;49mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_convert_token_to_id_with_added_voc(token))\n\u001b[1;32m    580\u001b[0m \u001b[39mreturn\u001b[39;00m ids\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 定义超参数\n",
    "device = torch.device('cuda')\n",
    "data_dir = './RACE/'\n",
    "batch_size = 8\n",
    "num_epochs = 5\n",
    "learning_rate = 2e-5\n",
    "\n",
    "# 加载RACE数据集\n",
    "train_dataset = RACEDataset(os.path.join(data_dir, 'train'))\n",
    "print('Train dataset done')\n",
    "dev_dataset = RACEDataset(os.path.join(data_dir, 'dev'))\n",
    "print('Dev dataset done')\n",
    "\n",
    "# 创建DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "print('Train dataloader done')\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=batch_size)\n",
    "print('Dev dataloader done')\n",
    "\n",
    "# 创建RoBERTa模型\n",
    "model = RoBERTaClassifier().to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 训练和评估模型\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    train_model(model, train_loader, criterion, optimizer, device)\n",
    "    accuracy = evaluate_model(model, dev_loader, device)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}: Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_dataset[0][2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('race')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ff8ee494585a1875a05112a68bddbbfd9aa592c4e1f7ee722c0946aa5b088794"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
